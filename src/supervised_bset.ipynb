{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy, os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from boundary_tree import *\n",
    "\n",
    "from tensorflow_utils import *\n",
    "from general import *\n",
    "from data_ops import *\n",
    "import logging_utils as logging\n",
    "\n",
    "logger = logging.setup_logger(file='../logs/supbset.log')\n",
    "cache_dir = '../cache'\n",
    "\n",
    "    \n",
    "class BasicNNet:\n",
    "    def __init__(self, dim_data, dim_layers, dim_pred, dropout, params_init):\n",
    "        self.dim_pred = dim_pred\n",
    "        self.data = tf.placeholder(\"float\", [None, dim_data])\n",
    "        self.pred, self.weight_loss, self.params = BasicNNet.build_model(\n",
    "            self.data, dim_layers, dim_pred, dropout, params_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_model(data, dim_layers, dim_pred, keep_prob, params_init):\n",
    "        params = []\n",
    "        z = None\n",
    "        weight_loss = tf.constant([0.])\n",
    "        data_prev = data\n",
    "        for dim_layer, index in zip(dim_layers + [dim_pred], range(len(dim_layers) + 1)):\n",
    "            z, w, b = BasicNNet.build_layer(data_prev, dim_layer, None if params_init is None else params_init[index])\n",
    "            params.append((w, b))\n",
    "            weight_loss += tf.reduce_sum(tf.square(w))\n",
    "            act = tf.nn.relu(z)\n",
    "            if index>0 and keep_prob is not None:\n",
    "                data_prev = tf.nn.dropout(act, keep_prob)\n",
    "            else:\n",
    "                data_prev = act\n",
    "\n",
    "        return z, weight_loss, params\n",
    "\n",
    "    @staticmethod\n",
    "    def build_layer(data_prev, dim_layer, lay_init):\n",
    "        if lay_init is None:\n",
    "            dim_layer_prev = data_prev.get_shape().as_list()[1]\n",
    "            std_dev = tf.sqrt(3./(dim_layer_prev + dim_layer))\n",
    "            w_init = tf.truncated_normal([dim_layer_prev, dim_layer], stddev=std_dev)\n",
    "            b_init = tf.zeros([dim_layer])  # random_normal\n",
    "        else:\n",
    "            w_init, b_init = lay_init\n",
    "\n",
    "        w = tf.Variable(w_init)\n",
    "        b = tf.Variable(b_init)\n",
    "        z = tf.matmul(data_prev, w) + b\n",
    "        return z, w, b\n",
    "\n",
    "    \n",
    "class BoundaryModel:\n",
    "    def __init__(self, dim_data, dim_layers, dim_inter, dim_pred, sigma, dropout, params_init):\n",
    "        self.debug = []\n",
    "        self.dim_inter = dim_inter\n",
    "        self.dim_pred = dim_pred\n",
    "        self.__nnet = BasicNNet(dim_data, dim_layers, dim_inter, dropout, params_init)\n",
    "        self.bnd_data = tf.placeholder(\"float\", [None, dim_inter])\n",
    "        self.bnd_target = tf.placeholder(\"float\", [None, dim_pred])\n",
    "        self.pred = self.calc_pred(self.inter, self.bnd_data, self.bnd_target, sigma)\n",
    "        \n",
    "    def calc_pred(self, data, bnd_data, bnd_target, sigma):\n",
    "        # pairwise distance matrix\n",
    "        q = tf.expand_dims(data, 1)\n",
    "        m = tf.tile(q, [1, tf.shape(bnd_data)[0], 1]) - bnd_data\n",
    "        s = tf.reduce_sum(tf.square(m), 2)\n",
    "        dists2 = s\n",
    "        dists = tf.sqrt(s)\n",
    "        \n",
    "        # gradient of py_func is none\n",
    "        # https://stackoverflow.com/questions/41535347/how-gradient-passed-by-tf-py-func\n",
    "        # d = tf.py_func(cdist, [data, bnd_data], tf.float32)\n",
    "\n",
    "        # class probability matrix\n",
    "        sm = tf.nn.softmax(-dists2/sigma)\n",
    "        p = tf.matmul(sm, bnd_target)\n",
    "        # self.debug.extend([data, bnd_data, bnd_target, dists, dists2, tf.nn.softmax(-dists), sm])\n",
    "        # self.debug.extend([s, tf.reduce_prod(tf.sign(s), [0,1])])\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.__nnet.data\n",
    "    @property\n",
    "    def inter(self): return self.__nnet.pred\n",
    "    @property\n",
    "    def weight_loss(self): return self.__nnet.weight_loss\n",
    "    @property\n",
    "    def params(self): return self.__nnet.params\n",
    "\n",
    "    \n",
    "class BasicTrainer(object):\n",
    "    def __init__(self, model, regularizer, learning_rate, train_data, train_labels, batch_size,\n",
    "                 test_data, test_labels, dump_freq, optimizer=None, decrease_learning_rate_at=[]):\n",
    "        self.model = model\n",
    "        self.debug = []\n",
    "        self.target = tf.placeholder(\"float\", [None, model.dim_pred])\n",
    "        self.is_correct = tf.equal(tf.argmax(model.pred, 1), tf.argmax(self.target, 1))\n",
    "        # self.debug.append(self.target)\n",
    "        self.test_error = 100*(1 - tf.reduce_mean(tf.cast(self.is_correct, \"float\")))\n",
    "\n",
    "        # calculate loss\n",
    "        self.entropy_loss = self.calc_entropy_loss(model.pred, self.target)\n",
    "        total_loss = self.entropy_loss + regularizer*self.model.weight_loss\n",
    "        self.learning_rate_var = tf.Variable(learning_rate, trainable=False)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decrease_learning_rate_at = decrease_learning_rate_at\n",
    "        self.optimizer = (optimizer if optimizer is not None else \n",
    "                          tf.train.AdamOptimizer)(learning_rate=self.learning_rate_var).minimize(total_loss)\n",
    "        #vself.debug.append([self.entropy_loss, model.pred, tf.train.AdamOptimizer(learning_rate=learning_rate).compute_gradients(self.entropy_loss)])\n",
    "        \n",
    "        self.trainingPipe = DataSet(train_data, train_labels)\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        self.dump_freq = dump_freq\n",
    "\n",
    "        self.history = []\n",
    "        self.last_epoch = -1\n",
    "        self.last_state = None\n",
    "        \n",
    "    def train(self, n_epochs, sess):\n",
    "        logger.info(\"Starting training.\" if self.trainingPipe.epochs_completed<1\n",
    "                    else \"Continuing from epoch %d.\"%self.trainingPipe.epochs_completed)\n",
    "        try:\n",
    "            while self.trainingPipe.epochs_completed < n_epochs:\n",
    "                m = self.model\n",
    "\n",
    "                batch_data, batch_labels = self.trainingPipe.next_batch(self.batch_size)\n",
    "                #self.test_data, self.test_labels = batch_data, batch_labels\n",
    "                test_feed_dict = {m.data: self.test_data, self.target: convert_one_hot(self.test_labels, m.dim_pred)}\n",
    "                train_feed_dict = {m.data: batch_data, self.target: convert_one_hot(batch_labels, m.dim_pred),\n",
    "                                   self.learning_rate_var:self.learning_rate}\n",
    "                self.setup_train_step(test_feed_dict, train_feed_dict, sess)\n",
    "                _, entropy_loss = sess.run([self.optimizer, self.entropy_loss], train_feed_dict)\n",
    "\n",
    "                if self.trainingPipe.epochs_completed > self.last_epoch:\n",
    "                    self.last_epoch = self.trainingPipe.epochs_completed\n",
    "                    is_correct, test_error = sess.run([self.is_correct, self.test_error], test_feed_dict)\n",
    "                    #self.logger.debug(sess.run(self.debug, train_feed_dict))\n",
    "                    logger.debug(\"Epoch: %d, Loss: %.3f, Pred error: %.3f\" % (self.last_epoch, entropy_loss, test_error))\n",
    "                    params = [sess.run(lay) for lay in m.params]\n",
    "                    state_lite = {'entropy_loss': entropy_loss, 'test_error': test_error}\n",
    "                    state_full = {'train_data': self.train_data, 'train_labels': self.train_labels,\n",
    "                                  'test_data': self.test_data, 'test_labels': self.test_labels,\n",
    "                                  'is_correct': is_correct, 'params': params}\n",
    "                    state_full.update(state_lite)\n",
    "                    self.setup_states(state_lite, state_full, sess)\n",
    "                    self.history.append(state_lite)\n",
    "                    self.last_state = state_full\n",
    "                    if self.last_epoch % self.dump_freq == 0:\n",
    "                        dump_pik('%s/temp_state_%d.pik' %(cache_dir, self.last_epoch), state_full)\n",
    "                    if self.last_epoch in self.decrease_learning_rate_at:\n",
    "                        logger.debug(\"Old learning_rate: %f\"%self.learning_rate)\n",
    "                        self.learning_rate /= 10.0\n",
    "                        logger.debug(\"New learning_rate: %f\"%self.learning_rate)\n",
    "            logger.info(\"Finished training.\")\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Interrupt received.\")\n",
    "    \n",
    "        return {'history': self.history, 'last_state': self.last_state}\n",
    "\n",
    "        \n",
    "class BasicNNetTrainer(BasicTrainer):\n",
    "    def calc_entropy_loss(self, pred, target):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=target))\n",
    "\n",
    "    def setup_train_step(self, test_feed_dict, train_feed_dict, sess):\n",
    "        pass\n",
    "\n",
    "    def setup_states(self, state_lite, state_full, sess):\n",
    "        train_trans = sess.run(self.model.pred, feed_dict={self.model.data: self.train_data})\n",
    "        test_trans = sess.run(self.model.pred, feed_dict={self.model.data: self.test_data})\n",
    "        state_full.update({'train_trans': train_trans, 'test_trans': test_trans})\n",
    "\n",
    "    \n",
    "class BoundaryTrainer(BasicTrainer):\n",
    "    def __init__(self, model, regularizer, learning_rate, train_data, train_labels,\n",
    "                 set_size, batch_size, test_data, test_labels, dump_freq, optimizer=None, decrease_learning_rate_at=[]):\n",
    "        super(BoundaryTrainer, self).__init__(model, regularizer, learning_rate, train_data, train_labels,\n",
    "                         batch_size, test_data, test_labels, dump_freq, optimizer, decrease_learning_rate_at)\n",
    "        self.set_size = set_size\n",
    "        self.temp_data = None\n",
    "        self.debug.extend(model.debug)\n",
    "\n",
    "    def calc_entropy_loss(self, pred, target):\n",
    "        # https://stackoverflow.com/questions/33712178/tensorflow-nan-bug\n",
    "        cor = tf.clip_by_value(pred,1e-8,1.0) # pred + 1e-8\n",
    "        ttf = target * tf.log(cor)\n",
    "        el = -tf.reduce_sum(ttf, [1])\n",
    "        #eq = tf.equal(target, 1)\n",
    "        #locs = tf.where(eq)\n",
    "        #locs = tf.cast(locs, tf.int32)\n",
    "        #ttf = tf.gather_nd(pred, locs)\n",
    "        #el = -tf.log(ttf)\n",
    "        return tf.reduce_mean(el)\n",
    "\n",
    "    def setup_train_step(self, test_feed_dict, train_feed_dict, sess):\n",
    "        set_data, set_labels = self.trainingPipe.next_batch(self.set_size)\n",
    "        set_inter = sess.run(self.model.inter, feed_dict={self.model.data: set_data})\n",
    "        b_set = build_boundary_set(set_inter, set_labels)\n",
    "        feed_dict = {self.model.bnd_data: b_set.values, self.model.bnd_target:\n",
    "                     convert_one_hot(b_set.labels, self.model.dim_pred)}\n",
    "        test_feed_dict.update(feed_dict)\n",
    "        train_feed_dict.update(feed_dict)\n",
    "        self.temp_data = {'set_data': set_data, 'set_labels': set_labels,\n",
    "                          'set_inter': set_inter, 'b_set': b_set}\n",
    "\n",
    "    def setup_states(self, state_lite, state_full, sess):\n",
    "        b_set = self.temp_data['b_set']\n",
    "        logger.debug(\"Set size: %d\" % b_set.size)\n",
    "        train_trans = sess.run(self.model.inter, feed_dict={self.model.data: self.train_data})\n",
    "        test_trans = sess.run(self.model.inter, feed_dict={self.model.data: self.test_data})\n",
    "        state_lite.update({'b_set_size': b_set.size})\n",
    "        state_full.update({'b_set_size': b_set.size, 'train_trans': train_trans, 'test_trans': test_trans})\n",
    "        state_full.update(self.temp_data)\n",
    "        self.temp_data = None\n",
    "    \n",
    "\n",
    "def get_hash_path(a):\n",
    "    a.flags.writeable = False\n",
    "    h = hash(a.tobytes())\n",
    "    return cache_dir + '/' + str(h)\n",
    "\n",
    "\n",
    "def load_cache(d):\n",
    "    f = get_hash_path(d)\n",
    "    return load_pik(f) if os.path.isfile(f) else None\n",
    "\n",
    "    \n",
    "def save_cache(d, c): dump_pik(get_hash_path(d), c)\n",
    "def final_average(el): return np.mean(el[-int(len(el)*.2):])\n",
    "    \n",
    "def plot_TSNE__(data, labels, title):\n",
    "    trans = load_cache(data)\n",
    "    if trans is None:\n",
    "        tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "        trans = tsne.fit_transform(data)\n",
    "        save_cache(data, trans)\n",
    "\n",
    "    plt.scatter(trans[:, 0], trans[:, 1], marker='*', s=5, edgecolor='none', c=labels)\n",
    "    plt.ylabel(title)\n",
    "\n",
    "    for i in range(max(labels)+1):\n",
    "        indices = labels == i\n",
    "        center = np.average(trans[indices], 0)\n",
    "        plt.text(center[0], center[1], str(i), fontsize=20, weight=\"bold\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def prod_perf_curves(history):\n",
    "    ax1 = plt.subplot(311)\n",
    "    el = fetch_field(history, 'entropy_loss')\n",
    "    plt.plot(el)\n",
    "    plt.ylabel('Entropy loss')\n",
    "    plt.grid()\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "    ax2 = plt.subplot(312)\n",
    "    te = fetch_field(history, 'test_error')\n",
    "    plt.plot(te)\n",
    "    plt.ylabel('Test error')\n",
    "    plt.grid()\n",
    "\n",
    "    if is_field(history, 'b_set_size'):\n",
    "        ax3 = plt.subplot(313, sharex=ax1)\n",
    "        plt.plot(fetch_field(history, 'b_set_size'))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Boundary set size')\n",
    "        plt.grid()\n",
    "        plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "    else:\n",
    "        plt.xlabel('Epoch')\n",
    "\n",
    "    plt.show()\n",
    "    logger.info(\"Entropy loss: %.2f, Test error: %.2f\"%(final_average(el), final_average(te)))\n",
    "\n",
    "    \n",
    "def prod_tsne(state, subset):\n",
    "    train_data = state['train_data']\n",
    "    train_trans = state['train_trans']\n",
    "    train_labels = state['train_labels']\n",
    "    if subset>0:\n",
    "        d, t, l = train_data[:subset], train_trans[:subset], train_labels[:subset]\n",
    "    else:\n",
    "        d, t, l = train_data, train_trans, train_labels\n",
    "    plot_TSNE__(t, l, 'TSNE for transformed data')\n",
    "    plot_TSNE__(d, l, 'TSNE for original data')\n",
    "    \n",
    "\n",
    "def save_result(sim_id, result):\n",
    "    dump_pik('%s/%s_result.pik' %(cache_dir, sim_id), result)\n",
    "\n",
    "    \n",
    "def prod_report(report, subset):\n",
    "    prod_perf_curves(report['history'])\n",
    "    prod_tsne(report['last_state'], subset)\n",
    "\n",
    "\n",
    "xy__ = lambda data: (data[:, 0], data[:, 1])\n",
    "\n",
    "def prod_report_2d(report):\n",
    "    prod_perf_curves(report['history'])\n",
    "    state = report['last_state']\n",
    "    b_set = state['b_set'] if 'b_set' in state else None\n",
    "        \n",
    "    plt.ylabel('Original data')\n",
    "    plt.scatter(*xy__(state['train_data']), marker='.', s=5, c=state['train_labels'], edgecolor='none', label='Training data')\n",
    "    plt.scatter(*xy__(state['test_data']), marker='o', s=25, c=state['test_labels'], edgecolor='none', label='Test data')\n",
    "    if b_set:\n",
    "        plt.scatter(*xy__(state['set_data']), marker='x', s=25, c=state['set_labels'], edgecolor='none', label='Set data')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(*xy__(state['train_trans']), marker='.', s=5, c=state['train_labels'], edgecolor='none', label='Training data')\n",
    "    plt.scatter(*xy__(state['test_trans']), marker='o', s=25, c=state['test_labels'], edgecolor='none', label='Test data')\n",
    "    if b_set:\n",
    "        plt.ylabel('Transformed data with last boundary set')\n",
    "        plt.scatter(*xy__(state['set_inter']), marker='x', s=25, c=state['set_labels'], edgecolor='none', label='Set training data')\n",
    "        plt.scatter(*xy__(b_set.values), marker='s', s=40, facecolors='none', color='k', label='Set data')\n",
    "    else:\n",
    "        plt.ylabel('Transformed data')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "    plt.ylabel('Transformed test set in last state')\n",
    "    plt.scatter(*xy__(state['test_trans']), marker='.', s=25, c=state['test_labels'], edgecolor='none', label='Test data')\n",
    "    plt.scatter(*xy__(state['test_trans'][~state['is_correct']]), marker='o', s=40, facecolors='none', color='k', label='False positives')\n",
    "    if b_set:\n",
    "        plt.scatter(*xy__(b_set.values), marker='s', s=25, c=b_set.labels, edgecolor='none', label='Set data')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prod_trans_2d(report):\n",
    "    prod_perf_curves(report['history'])\n",
    "    state = report['last_state']\n",
    "    trans = state['train_trans']\n",
    "    labels = state['train_labels']\n",
    "    \n",
    "    plt.scatter(trans[:, 0], trans[:, 1], marker='*', s=5, edgecolor='none', c=labels)\n",
    "    for i in range(max(labels)+1):\n",
    "        indices = labels == i\n",
    "        center = np.average(trans[indices], 0)\n",
    "        plt.text(center[0], center[1], str(i), fontsize=20, weight=\"bold\")\n",
    "\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def prod_report_easy(sim_id, func, trainer, n_epochs, subset=0):\n",
    "    path = cache_dir + '/' + sim_id\n",
    "    if os.path.isfile(path):\n",
    "        logger.info(\"Loading cached training data [%s]\"%path)\n",
    "        result = load_pik(path)\n",
    "    else:\n",
    "        sess = setup_sess()\n",
    "        result = trainer.train(n_epochs, sess=sess)\n",
    "        dump_pik(path, result)\n",
    "    \n",
    "    if subset==0:\n",
    "        func(result)\n",
    "    else:\n",
    "        func(result, subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on neural net with gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BasicNNet(dim_data=2, dim_layers=[100, 100, 30], dim_pred=2, dropout=None, params_init=None)\n",
    "trainer = BasicNNetTrainer(model=model, regularizer=.001, learning_rate=.001,\n",
    "                           train_data=train_data, train_labels=train_labels, batch_size=10,\n",
    "                           test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                           optimizer=tf.train.GradientDescentOptimizer)\n",
    "prod_report_easy('hm_nn_gdo', prod_report_2d, trainer, n_epochs=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on neural net with gradient descent optimizer (with dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BasicNNet(dim_data=2, dim_layers=[100, 100, 30], dim_pred=2, dropout=.5, params_init=None)\n",
    "trainer = BasicNNetTrainer(model=model, regularizer=.001, learning_rate=.001,\n",
    "                           train_data=train_data, train_labels=train_labels, batch_size=10,\n",
    "                           test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                           optimizer=tf.train.GradientDescentOptimizer)\n",
    "prod_report_easy('hm_nn_gdo_do', prod_report_2d, trainer, n_epochs=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on neural net with gradient descent optimizer (with dropout except 1st layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BasicNNet(dim_data=2, dim_layers=[100, 100, 30], dim_pred=2, dropout=.5, params_init=None)\n",
    "trainer = BasicNNetTrainer(model=model, regularizer=.001, learning_rate=.001,\n",
    "                           train_data=train_data, train_labels=train_labels, batch_size=10,\n",
    "                           test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                           optimizer=tf.train.GradientDescentOptimizer)\n",
    "prod_report_easy('hm_nn_gdo_do_1', prod_report_2d, trainer, n_epochs=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on neural net with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BasicNNet(dim_data=2, dim_layers=[100, 100, 30], dim_pred=2, dropout=None, params_init=None)\n",
    "trainer = BasicNNetTrainer(model=model, regularizer=.001, learning_rate=.001,\n",
    "                           train_data=train_data, train_labels=train_labels, batch_size=10,\n",
    "                           test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                           optimizer=tf.train.AdamOptimizer)\n",
    "prod_report_easy('hm_nn_ado', prod_report_2d, trainer, n_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on neural net with Adam optimizer (with dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BasicNNet(dim_data=2, dim_layers=[100, 100, 30], dim_pred=2, dropout=.5, params_init=None)\n",
    "trainer = BasicNNetTrainer(model=model, regularizer=.001, learning_rate=.001,\n",
    "                           train_data=train_data, train_labels=train_labels, batch_size=10,\n",
    "                           test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                           optimizer=tf.train.AdamOptimizer)\n",
    "prod_report_easy('hm_nn_ado_do', prod_report_2d, trainer, n_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on neural net with Adam optimizer (with dropout except 1st layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BasicNNet(dim_data=2, dim_layers=[100, 100, 30], dim_pred=2, dropout=.5, params_init=None)\n",
    "trainer = BasicNNetTrainer(model=model, regularizer=.001, learning_rate=.001,\n",
    "                           train_data=train_data, train_labels=train_labels, batch_size=10,\n",
    "                           test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                           optimizer=tf.train.AdamOptimizer)\n",
    "prod_report_easy('hm_nn_ado_do_1', prod_report_2d, trainer, n_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on boundary set with gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BoundaryModel(dim_data=2, dim_layers=[100, 100, 30], dim_inter=2, dim_pred=2, sigma=1, dropout=None, params_init=None)\n",
    "trainer = BoundaryTrainer(model=model, regularizer=.005, learning_rate=.0001,\n",
    "                          train_data=train_data, train_labels=train_labels, set_size=20, batch_size=10,\n",
    "                          test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                          optimizer=tf.train.GradientDescentOptimizer)\n",
    "prod_report_easy('hm_bs_gdo', prod_report_2d, trainer, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on boundary set with gradient descent optimizer (with dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BoundaryModel(dim_data=2, dim_layers=[100, 100, 30], dim_inter=2, dim_pred=2, sigma=1, dropout=.5, params_init=None)\n",
    "trainer = BoundaryTrainer(model=model, regularizer=.005, learning_rate=.0001,\n",
    "                          train_data=train_data, train_labels=train_labels, set_size=20, batch_size=10,\n",
    "                          test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                          optimizer=tf.train.GradientDescentOptimizer)\n",
    "prod_report_easy('hm_bs_gdo_do', prod_report_2d, trainer, n_epochs=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on boundary set with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BoundaryModel(dim_data=2, dim_layers=[100, 100, 30], dim_inter=2, dim_pred=2, sigma=1, dropout=None, params_init=None)\n",
    "trainer = BoundaryTrainer(model=model, regularizer=.005, learning_rate=.0001,\n",
    "                          train_data=train_data, train_labels=train_labels, set_size=20, batch_size=10,\n",
    "                          test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                          optimizer=tf.train.AdamOptimizer)\n",
    "prod_report_easy('hm_bs_ado', prod_report_2d, trainer, n_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-moons on boundary set with Adam optimizer (with dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = make_half_moons(n_training=1000, n_test=50)\n",
    "model = BoundaryModel(dim_data=2, dim_layers=[100, 100, 30], dim_inter=2, dim_pred=2, sigma=1, dropout=.5, params_init=None)\n",
    "trainer = BoundaryTrainer(model=model, regularizer=.005, learning_rate=.0001,\n",
    "                          train_data=train_data, train_labels=train_labels, set_size=20, batch_size=10,\n",
    "                          test_data=test_data, test_labels=test_labels, dump_freq=100,\n",
    "                          optimizer=tf.train.AdamOptimizer)\n",
    "prod_report_easy('hm_bs_ado_do', prod_report_2d, trainer, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 2-class on neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = load_mnist_digits_2('tensorflow')\n",
    "model = BasicNNet(dim_data=train_data.shape[1], dim_layers=[400, 400, 20], dim_pred=2, dropout=None, params_init=None)\n",
    "trainer = BasicNNetTrainer(model, regularizer=.001, learning_rate=.001, train_data=train_data,\n",
    "                            train_labels=train_labels, batch_size=1000,\n",
    "                            test_data=test_data, test_labels=test_labels, dump_freq=100)\n",
    "prod_report_easy('mnist2_nn', prod_report, trainer, n_epochs=50, subset=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 2-class on boundary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = load_mnist_digits_2('tensorflow')\n",
    "model = BoundaryModel(dim_data=train_data.shape[1], dim_layers=[400, 400],\n",
    "                       dim_inter=20, dim_pred=2, sigma=1, dropout=None, params_init=None)\n",
    "trainer = BoundaryTrainer(model, regularizer=.001, learning_rate=.0001, train_data=train_data,\n",
    "                            train_labels=train_labels, set_size=1000, batch_size=1000,\n",
    "                            test_data=test_data, test_labels=test_labels, dump_freq=100)\n",
    "prod_report_easy('mnist2_bs', prod_report, trainer, n_epochs=1000, subset=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST on neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = load_mnist_digits('tensorflow')\n",
    "model = BasicNNet(dim_data=train_data.shape[1], dim_layers=[400, 400, 20],\n",
    "                   dim_pred=10, dropout=None, params_init=None)\n",
    "trainer = BasicNNetTrainer(model, regularizer=.001, learning_rate=.001, train_data=train_data,\n",
    "                            train_labels=train_labels, batch_size=1000,\n",
    "                            test_data=test_data, test_labels=test_labels, dump_freq=200)\n",
    "prod_report_easy('mnist_nn', prod_report, trainer, n_epochs=100, subset=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST on neural net with gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(1)\n",
    "train_data, train_labels, test_data, test_labels = load_mnist_digits('tensorflow')\n",
    "model = BasicNNet(dim_data=train_data.shape[1], dim_layers=[400, 400, 20],\n",
    "                   dim_pred=10, dropout=None, params_init=None)\n",
    "trainer = BasicNNetTrainer(model, regularizer=.001, learning_rate=.001, train_data=train_data,\n",
    "                            train_labels=train_labels, batch_size=1000,\n",
    "                            test_data=test_data, test_labels=test_labels, dump_freq=200,\n",
    "                            optimizer=tf.train.GradientDescentOptimizer)\n",
    "prod_report_easy('mnist_nn_gdo', prod_report, trainer, n_epochs=800, subset=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST on boundary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(565)\n",
    "#params_init = load_pik('state_300.pik')['params']\n",
    "#history = load_pik('temp/history.pik')\n",
    "train_data, train_labels, test_data, test_labels = load_mnist_digits('tensorflow')\n",
    "model = BoundaryModel(dim_data=train_data.shape[1], dim_layers=[400, 400],\n",
    "                       dim_inter=20, dim_pred=10, sigma=1, dropout=None, params_init=None)\n",
    "trainer = BoundaryTrainer(model, regularizer=.001, learning_rate=.0001, train_data=train_data,\n",
    "                            train_labels=train_labels, set_size=1000, batch_size=1000,\n",
    "                            test_data=test_data, test_labels=test_labels, dump_freq=200, decrease_learning_rate_at=[600, 4000])\n",
    "prod_report_easy('mnist_bs', prod_report, trainer, n_epochs=5000, subset=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(565)\n",
    "#params_init = load_pik('state_300.pik')['params']\n",
    "#history = load_pik('temp/history.pik')\n",
    "train_data, train_labels, test_data, test_labels = load_mnist_digits('tensorflow')\n",
    "model = BoundaryModel(dim_data=train_data.shape[1], dim_layers=[400, 400],\n",
    "                       dim_inter=20, dim_pred=10, sigma=1, dropout=None, params_init=None)\n",
    "trainer = BoundaryTrainer(model, regularizer=.001, learning_rate=.0001, train_data=train_data,\n",
    "                            train_labels=train_labels, set_size=1000, batch_size=1000,\n",
    "                            test_data=test_data, test_labels=test_labels, dump_freq=200)\n",
    "prod_report_easy('mnist_bs', prod_report, trainer, n_epochs=1000, subset=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same scatter plots as above with 20,000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_report_easy('mnist_bs', prod_report, trainer, n_epochs=1000, subset=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST on boundary set with gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(565)\n",
    "train_data, train_labels, test_data, test_labels = load_mnist_digits('tensorflow')\n",
    "model = BoundaryModel(dim_data=train_data.shape[1], dim_layers=[400, 400],\n",
    "                       dim_inter=20, dim_pred=10, sigma=1, dropout=None, params_init=None)\n",
    "trainer = BoundaryTrainer(model, regularizer=.001, learning_rate=.0001, train_data=train_data,\n",
    "                            train_labels=train_labels, set_size=1000, batch_size=1000,\n",
    "                            test_data=test_data, test_labels=test_labels, dump_freq=200,\n",
    "                            optimizer=tf.train.GradientDescentOptimizer)\n",
    "prod_report_easy('mnist_bs_gdo', prod_report, trainer, n_epochs=4000, subset=6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST on boundary set with intermediate dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_all(565)\n",
    "train_data, train_labels, test_data, test_labels = load_mnist_digits('tensorflow')\n",
    "model = BoundaryModel(dim_data=train_data.shape[1], dim_layers=[400, 400],\n",
    "                       dim_inter=2, dim_pred=10, sigma=1, dropout=None, params_init=None)\n",
    "trainer = BoundaryTrainer(model, regularizer=.001, learning_rate=.0001, train_data=train_data,\n",
    "                            train_labels=train_labels, set_size=1000, batch_size=1000,\n",
    "                            test_data=test_data, test_labels=test_labels, dump_freq=200)\n",
    "prod_report_easy('mnist_bs2', prod_trans_2d, trainer, n_epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py27tfGPU",
   "language": "python",
   "name": "py27tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
